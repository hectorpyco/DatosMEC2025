{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980be455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Cambia 'archivo.csv' por el nombre real de tus archivos\n",
    "df = pd.read_csv('titulos.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df.columns)\n",
    "print(df.describe(include='all'))\n",
    "# Para ver mejor la salida usar display\n",
    "from IPython.display import display\n",
    "display(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43928ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['tipo_institucion'].value_counts().plot(kind='barh')\n",
    "plt.xlabel(\"Cantidad\")\n",
    "plt.ylabel(\"Tipo de institución\")\n",
    "plt.title(\"Cantidad por tipo de institución\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa5846",
   "metadata": {},
   "source": [
    "**Detección temprana de posibles duplicados semánticos en campos categóricos**\n",
    "\n",
    "No existe un comando de pandas que \"adivine\" y te agrupe automáticamente las variantes, pero sí hay buenas prácticas y algunos trucos útiles:\n",
    "\n",
    "1. Ver valores únicos en minúsculas y comparar contra los originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a77b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = set(df['tipo_institucion'].unique())\n",
    "normalized = set(df['tipo_institucion'].str.lower().str.strip().unique())\n",
    "print(f\"Número de categorías originales: {len(orig)}\")\n",
    "print(f\"Número de categorías normalizadas: {len(normalized)}\")\n",
    "if len(orig) != len(normalized):\n",
    "    print(\"¡Atención: hay posibles duplicados por diferencias de mayúsculas/minúsculas o espacios!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3240b64",
   "metadata": {},
   "source": [
    "2. Mostrar “grupos” de valores que solo difieren por casing/espacios.\n",
    "Esto te muestra exactamente qué valores únicos serían agrupados si tratas todo igual (útil para \"ver\" los falsos duplicados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un mapeo de valores originales agrupados por su versión normalizada\n",
    "from collections import defaultdict\n",
    "groups = defaultdict(set)\n",
    "for val in df['tipo_institucion'].unique():\n",
    "    groups[str(val).strip().lower()].add(val)\n",
    "# Filtra los que tienen más de una variante:\n",
    "duplicados = {norm: variants for norm, variants in groups.items() if len(variants) > 1}\n",
    "print(duplicados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d696e8a",
   "metadata": {},
   "source": [
    "3. Limpiar y ver Estadísticas rápidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tipo_institucion_normalizado'] = df['tipo_institucion'].str.lower().str.strip()\n",
    "print(df['tipo_institucion_normalizado'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dcdeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tipo_institucion_normalizado'].value_counts().plot(kind='barh')\n",
    "plt.xlabel(\"Cantidad\")\n",
    "plt.ylabel(\"Tipo de institución\")\n",
    "plt.title(\"Cantidad por tipo de institución normalizado\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1444ac1",
   "metadata": {},
   "source": [
    "*Función para detectar posibles duplicados en todas las columnas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_duplicados_categoricos(df):\n",
    "    \"\"\"\n",
    "    Detecta posibles duplicados por diferencias de mayúsculas/minúsculas \n",
    "    y espacios en todas las columnas categóricas del DataFrame\n",
    "    \"\"\"\n",
    "    print(\"=== ANÁLISIS DE DUPLICADOS CATEGÓRICOS ===\\n\")\n",
    "    \n",
    "    # Filtrar solo columnas de tipo object (categóricas/string)\n",
    "    columnas_categoricas = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    problemas_encontrados = []\n",
    "    \n",
    "    for columna in columnas_categoricas:\n",
    "        print(f\"📊 Analizando: {columna}\")\n",
    "        \n",
    "        # Obtener valores únicos originales y normalizados\n",
    "        orig = set(df[columna].dropna().unique())\n",
    "        # Normalización extendida: minúsculas + espacios + acentos\n",
    "        normalized = set(\n",
    "            df[columna]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .str.lower()\n",
    "            .str.strip()\n",
    "            .str.normalize('NFKD')\n",
    "            .str.encode('ascii', 'ignore')\n",
    "            .str.decode('utf-8')\n",
    "            .unique()\n",
    "        )\n",
    "        \n",
    "        print(f\"   Categorías originales: {len(orig)}\")\n",
    "        print(f\"   Categorías normalizadas: {len(normalized)}\")\n",
    "        \n",
    "        if len(orig) != len(normalized):\n",
    "            diferencia = len(orig) - len(normalized)\n",
    "            print(f\"   ⚠️  ATENCIÓN: {diferencia} posibles duplicados detectados!\")\n",
    "            problemas_encontrados.append((columna, diferencia))\n",
    "        else:\n",
    "            print(\"   ✅ Sin duplicados detectados\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Resumen final\n",
    "    if problemas_encontrados:\n",
    "        print(\"\\n🚨 RESUMEN DE PROBLEMAS ENCONTRADOS:\")\n",
    "        for columna, cantidad in problemas_encontrados:\n",
    "            print(f\"   • {columna}: {cantidad} duplicados potenciales\")\n",
    "    else:\n",
    "        print(\"\\n✅ No se encontraron duplicados en ninguna columna categórica\")\n",
    "\n",
    "# Ejecutar la función\n",
    "detectar_duplicados_categoricos(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2068574",
   "metadata": {},
   "source": [
    "**Técnicas más avanzadas para detectar duplicados \"semánticos\" que van más allá de la normalización básica:**\n",
    "\n",
    "Función \"integradora avanzada\" adaptada para recorrer todas las columnas categóricas del DataFrame y aplicar fuzzy matching, abreviación y clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9bb92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- Utilidades --------------------\n",
    "\n",
    "def _normalizar_valores(valores, quitar_acentos=True):\n",
    "    s = pd.Series(valores, dtype=\"object\").astype(str).str.lower().str.strip()\n",
    "    if quitar_acentos:\n",
    "        s = (s\n",
    "             .str.normalize('NFKD')\n",
    "             .str.encode('ascii', 'ignore')\n",
    "             .str.decode('utf-8'))\n",
    "    return s.tolist()\n",
    "\n",
    "def _estimacion_tiempo_pairs(n, k_ref=120_000, base=0.0):\n",
    "    \"\"\"\n",
    "    Estima tiempo en segundos para comparar ~nC2 pares.\n",
    "    k_ref controla la escala (ajústalo según tu máquina).\n",
    "    \"\"\"\n",
    "    pares = n * (n - 1) / 2\n",
    "    return base + (pares / k_ref)\n",
    "\n",
    "# -------------------- Técnicas --------------------\n",
    "\n",
    "def detectar_similares_fuzzy(valores, umbral=85, max_pairs=None):\n",
    "    similares = []\n",
    "    valores_unicos = list(dict.fromkeys(valores))  # preserva orden\n",
    "    combos = itertools.combinations(valores_unicos, 2)\n",
    "\n",
    "    if max_pairs is not None:\n",
    "        combos = itertools.islice(combos, max_pairs)\n",
    "\n",
    "    for val1, val2 in combos:\n",
    "        ratio = fuzz.ratio(str(val1), str(val2))\n",
    "        if ratio >= umbral:\n",
    "            similares.append((val1, val2, ratio))\n",
    "    return similares\n",
    "\n",
    "def detectar_abreviaciones(valores):\n",
    "    abreviaciones = []\n",
    "    valores_unicos = list(dict.fromkeys(valores))\n",
    "    for val1, val2 in itertools.combinations(valores_unicos, 2):\n",
    "        str1, str2 = str(val1).lower(), str(val2).lower()\n",
    "        if str1 in str2 or str2 in str1:\n",
    "            abreviaciones.append((val1, val2))\n",
    "        iniciales1 = ''.join([w[0] for w in str1.split() if w])\n",
    "        iniciales2 = ''.join([w[0] for w in str2.split() if w])\n",
    "        if iniciales1 == str2 or iniciales2 == str1:\n",
    "            abreviaciones.append((val1, val2))\n",
    "    return abreviaciones\n",
    "\n",
    "def agrupar_similares_automatico(valores, eps=0.3, ngram_range=(2,3)):\n",
    "    vec = TfidfVectorizer(analyzer='char_wb', ngram_range=ngram_range)\n",
    "    X = vec.fit_transform([str(v) for v in valores])\n",
    "    clustering = DBSCAN(eps=eps, min_samples=2, metric='cosine')\n",
    "    labels = clustering.fit_predict(X)\n",
    "    grupos = defaultdict(list)\n",
    "    for i, label in enumerate(labels):\n",
    "        if label != -1:\n",
    "            grupos[int(label)].append(valores[i])\n",
    "    return grupos\n",
    "\n",
    "# -------------------- Función integradora --------------------\n",
    "\n",
    "def diagnostico_avanzado_duplicados_allcols(\n",
    "    df,\n",
    "    tecnicas=('fuzzy', 'abrev', 'cluster'),\n",
    "    fuzzy_umbral=85,\n",
    "    max_unicos_auto=250,          # por encima de esto, se considera \"pesada\"\n",
    "    max_pairs_fuzzy=200_000,      # límite duro de pares a evaluar con fuzzy\n",
    "    normalizar=True,              # normalización textual previa\n",
    "    quitar_acentos=True,          # eliminar acentos en normalización\n",
    "    top_resultados=3,             # cantidad de ejemplos para imprimir por técnica\n",
    "    eps_cluster=0.3,              # parámetro de DBSCAN\n",
    "    ngram_range_cluster=(2,3)     # n-gramas para TF-IDF de clustering\n",
    "):\n",
    "    \"\"\"\n",
    "    Diagnóstico avanzado de duplicados en TODAS las columnas categóricas.\n",
    "    - Evita cálculos costosos en columnas con demasiados únicos (salta con aviso).\n",
    "    - Estima tiempo y limita pares a evaluar para fuzzy.\n",
    "    - Aplica normalización (lower/strip/acentos) de forma opcional.\n",
    "    \"\"\"\n",
    "    columnas_cat = df.select_dtypes(include=['object']).columns\n",
    "    print(\"=== DIAGNÓSTICO AVANZADO DE DUPLICADOS (todas las columnas categóricas) ===\\n\")\n",
    "\n",
    "    pesadas = []\n",
    "    for col in columnas_cat:\n",
    "        vals = df[col].dropna().unique().tolist()\n",
    "        n = len(vals)\n",
    "        print(f\"\\n🔍 Columna: {col}  |  únicos: {n}\")\n",
    "\n",
    "        if normalizar:\n",
    "            vals_norm = _normalizar_valores(vals, quitar_acentos=quitar_acentos)\n",
    "        else:\n",
    "            vals_norm = [str(v) for v in vals]\n",
    "\n",
    "        # Estimación de costo\n",
    "        t_est = _estimacion_tiempo_pairs(n)\n",
    "        print(f\"   ⏱️ Estimación rápida (fuzzy ~O(n²)): ~{t_est:.1f}s\")\n",
    "\n",
    "        # Salto automático si es muy pesada\n",
    "        if n > max_unicos_auto:\n",
    "            print(f\"   ⏭️  Saltada: demasiados valores únicos (> {max_unicos_auto}).\")\n",
    "            pesadas.append((col, n))\n",
    "            continue\n",
    "\n",
    "        # Técnicas\n",
    "        if 'fuzzy' in tecnicas:\n",
    "            start = time.time()\n",
    "            # Limitar pares si hace falta\n",
    "            max_pairs = max_pairs_fuzzy\n",
    "            resultados = detectar_similares_fuzzy(vals_norm, umbral=fuzzy_umbral, max_pairs=max_pairs)\n",
    "            dur = time.time() - start\n",
    "            if resultados:\n",
    "                print(f\"   📊 Fuzzy (≥{fuzzy_umbral}%) → {len(resultados)} pares  |  {dur:.1f}s\")\n",
    "                for v1, v2, sc in resultados[:top_resultados]:\n",
    "                    print(f\"      {sc}%: '{v1}' ≈ '{v2}'\")\n",
    "            else:\n",
    "                print(f\"   📊 Fuzzy: sin pares sobre el umbral  |  {dur:.1f}s\")\n",
    "\n",
    "        if 'abrev' in tecnicas:\n",
    "            start = time.time()\n",
    "            abrev = detectar_abreviaciones(vals_norm)\n",
    "            dur = time.time() - start\n",
    "            if abrev:\n",
    "                print(f\"   🔤 Abreviaciones/contenimientos → {len(abrev)} pares  |  {dur:.1f}s\")\n",
    "                for v1, v2 in abrev[:top_resultados]:\n",
    "                    print(f\"      '{v1}' ↔ '{v2}'\")\n",
    "            else:\n",
    "                print(f\"   🔤 Abreviaciones: no se detectaron  |  {dur:.1f}s\")\n",
    "\n",
    "        if 'cluster' in tecnicas:\n",
    "            start = time.time()\n",
    "            grupos = agrupar_similares_automatico(vals_norm, eps=eps_cluster, ngram_range=ngram_range_cluster)\n",
    "            dur = time.time() - start\n",
    "            if grupos:\n",
    "                print(f\"   🎯 Clustering (DBSCAN) → {len(grupos)} grupos  |  {dur:.1f}s\")\n",
    "                for i, (g, items) in enumerate(grupos.items()):\n",
    "                    if i < top_resultados:\n",
    "                        print(f\"      Grupo {g}: {items}\")\n",
    "            else:\n",
    "                print(f\"   🎯 Clustering: no se formaron grupos  |  {dur:.1f}s\")\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    if pesadas:\n",
    "        print(\"\\n⚠️ Columnas saltadas por ser muy pesadas (ajusta max_unicos_auto si deseas incluirlas):\")\n",
    "        for col, n in pesadas:\n",
    "            print(f\"   • {col}: {n} únicos\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "diagnostico_avanzado_duplicados_allcols(\n",
    "     df,\n",
    "     tecnicas=('fuzzy', 'abrev', 'cluster'),\n",
    "     fuzzy_umbral=85,\n",
    "     max_unicos_auto=1000,\n",
    "     max_pairs_fuzzy=200_000,\n",
    "     normalizar=True,\n",
    "     quitar_acentos=True,\n",
    "     top_resultados=3,\n",
    "     eps_cluster=0.3,\n",
    "     ngram_range_cluster=(2,3)\n",
    " )\n",
    "\"\"\" Sugerencias:\n",
    "\n",
    "Ajusta max_unicos_auto según tu hardware. 250–400 suele ser razonable.\n",
    "\n",
    "Si quieres incluir “documento”, sube max_unicos_auto pero baja max_pairs_fuzzy para acotar tiempo.\n",
    "\n",
    "La estimación es orientativa; puedes calibrar k_ref para tu equipo. \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
