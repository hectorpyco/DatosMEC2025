{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980be455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Cambia 'archivo.csv' por el nombre real de tus archivos\n",
    "df = pd.read_csv('titulos.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(df.columns)\n",
    "print(df.describe(include='all'))\n",
    "# Para ver mejor la salida usar display\n",
    "from IPython.display import display\n",
    "display(df.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43928ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['tipo_institucion'].value_counts().plot(kind='barh')\n",
    "plt.xlabel(\"Cantidad\")\n",
    "plt.ylabel(\"Tipo de instituciÃ³n\")\n",
    "plt.title(\"Cantidad por tipo de instituciÃ³n\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa5846",
   "metadata": {},
   "source": [
    "**DetecciÃ³n temprana de posibles duplicados semÃ¡nticos en campos categÃ³ricos**\n",
    "\n",
    "No existe un comando de pandas que \"adivine\" y te agrupe automÃ¡ticamente las variantes, pero sÃ­ hay buenas prÃ¡cticas y algunos trucos Ãºtiles:\n",
    "\n",
    "1. Ver valores Ãºnicos en minÃºsculas y comparar contra los originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a77b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = set(df['tipo_institucion'].unique())\n",
    "normalized = set(df['tipo_institucion'].str.lower().str.strip().unique())\n",
    "print(f\"NÃºmero de categorÃ­as originales: {len(orig)}\")\n",
    "print(f\"NÃºmero de categorÃ­as normalizadas: {len(normalized)}\")\n",
    "if len(orig) != len(normalized):\n",
    "    print(\"Â¡AtenciÃ³n: hay posibles duplicados por diferencias de mayÃºsculas/minÃºsculas o espacios!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3240b64",
   "metadata": {},
   "source": [
    "2. Mostrar â€œgruposâ€ de valores que solo difieren por casing/espacios.\n",
    "Esto te muestra exactamente quÃ© valores Ãºnicos serÃ­an agrupados si tratas todo igual (Ãºtil para \"ver\" los falsos duplicados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un mapeo de valores originales agrupados por su versiÃ³n normalizada\n",
    "from collections import defaultdict\n",
    "groups = defaultdict(set)\n",
    "for val in df['tipo_institucion'].unique():\n",
    "    groups[str(val).strip().lower()].add(val)\n",
    "# Filtra los que tienen mÃ¡s de una variante:\n",
    "duplicados = {norm: variants for norm, variants in groups.items() if len(variants) > 1}\n",
    "print(duplicados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d696e8a",
   "metadata": {},
   "source": [
    "3. Limpiar y ver EstadÃ­sticas rÃ¡pidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tipo_institucion_normalizado'] = df['tipo_institucion'].str.lower().str.strip()\n",
    "print(df['tipo_institucion_normalizado'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dcdeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tipo_institucion_normalizado'].value_counts().plot(kind='barh')\n",
    "plt.xlabel(\"Cantidad\")\n",
    "plt.ylabel(\"Tipo de instituciÃ³n\")\n",
    "plt.title(\"Cantidad por tipo de instituciÃ³n normalizado\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1444ac1",
   "metadata": {},
   "source": [
    "*FunciÃ³n para detectar posibles duplicados en todas las columnas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectar_duplicados_categoricos(df):\n",
    "    \"\"\"\n",
    "    Detecta posibles duplicados por diferencias de mayÃºsculas/minÃºsculas \n",
    "    y espacios en todas las columnas categÃ³ricas del DataFrame\n",
    "    \"\"\"\n",
    "    print(\"=== ANÃLISIS DE DUPLICADOS CATEGÃ“RICOS ===\\n\")\n",
    "    \n",
    "    # Filtrar solo columnas de tipo object (categÃ³ricas/string)\n",
    "    columnas_categoricas = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    problemas_encontrados = []\n",
    "    \n",
    "    for columna in columnas_categoricas:\n",
    "        print(f\"ðŸ“Š Analizando: {columna}\")\n",
    "        \n",
    "        # Obtener valores Ãºnicos originales y normalizados\n",
    "        orig = set(df[columna].dropna().unique())\n",
    "        # NormalizaciÃ³n extendida: minÃºsculas + espacios + acentos\n",
    "        normalized = set(\n",
    "            df[columna]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .str.lower()\n",
    "            .str.strip()\n",
    "            .str.normalize('NFKD')\n",
    "            .str.encode('ascii', 'ignore')\n",
    "            .str.decode('utf-8')\n",
    "            .unique()\n",
    "        )\n",
    "        \n",
    "        print(f\"   CategorÃ­as originales: {len(orig)}\")\n",
    "        print(f\"   CategorÃ­as normalizadas: {len(normalized)}\")\n",
    "        \n",
    "        if len(orig) != len(normalized):\n",
    "            diferencia = len(orig) - len(normalized)\n",
    "            print(f\"   âš ï¸  ATENCIÃ“N: {diferencia} posibles duplicados detectados!\")\n",
    "            problemas_encontrados.append((columna, diferencia))\n",
    "        else:\n",
    "            print(\"   âœ… Sin duplicados detectados\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Resumen final\n",
    "    if problemas_encontrados:\n",
    "        print(\"\\nðŸš¨ RESUMEN DE PROBLEMAS ENCONTRADOS:\")\n",
    "        for columna, cantidad in problemas_encontrados:\n",
    "            print(f\"   â€¢ {columna}: {cantidad} duplicados potenciales\")\n",
    "    else:\n",
    "        print(\"\\nâœ… No se encontraron duplicados en ninguna columna categÃ³rica\")\n",
    "\n",
    "# Ejecutar la funciÃ³n\n",
    "detectar_duplicados_categoricos(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2068574",
   "metadata": {},
   "source": [
    "**TÃ©cnicas mÃ¡s avanzadas para detectar duplicados \"semÃ¡nticos\" que van mÃ¡s allÃ¡ de la normalizaciÃ³n bÃ¡sica:**\n",
    "\n",
    "FunciÃ³n \"integradora avanzada\" adaptada para recorrer todas las columnas categÃ³ricas del DataFrame y aplicar fuzzy matching, abreviaciÃ³n y clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9bb92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- Utilidades --------------------\n",
    "\n",
    "def _normalizar_valores(valores, quitar_acentos=True):\n",
    "    s = pd.Series(valores, dtype=\"object\").astype(str).str.lower().str.strip()\n",
    "    if quitar_acentos:\n",
    "        s = (s\n",
    "             .str.normalize('NFKD')\n",
    "             .str.encode('ascii', 'ignore')\n",
    "             .str.decode('utf-8'))\n",
    "    return s.tolist()\n",
    "\n",
    "def _estimacion_tiempo_pairs(n, k_ref=120_000, base=0.0):\n",
    "    \"\"\"\n",
    "    Estima tiempo en segundos para comparar ~nC2 pares.\n",
    "    k_ref controla la escala (ajÃºstalo segÃºn tu mÃ¡quina).\n",
    "    \"\"\"\n",
    "    pares = n * (n - 1) / 2\n",
    "    return base + (pares / k_ref)\n",
    "\n",
    "# -------------------- TÃ©cnicas --------------------\n",
    "\n",
    "def detectar_similares_fuzzy(valores, umbral=85, max_pairs=None):\n",
    "    similares = []\n",
    "    valores_unicos = list(dict.fromkeys(valores))  # preserva orden\n",
    "    combos = itertools.combinations(valores_unicos, 2)\n",
    "\n",
    "    if max_pairs is not None:\n",
    "        combos = itertools.islice(combos, max_pairs)\n",
    "\n",
    "    for val1, val2 in combos:\n",
    "        ratio = fuzz.ratio(str(val1), str(val2))\n",
    "        if ratio >= umbral:\n",
    "            similares.append((val1, val2, ratio))\n",
    "    return similares\n",
    "\n",
    "def detectar_abreviaciones(valores):\n",
    "    abreviaciones = []\n",
    "    valores_unicos = list(dict.fromkeys(valores))\n",
    "    for val1, val2 in itertools.combinations(valores_unicos, 2):\n",
    "        str1, str2 = str(val1).lower(), str(val2).lower()\n",
    "        if str1 in str2 or str2 in str1:\n",
    "            abreviaciones.append((val1, val2))\n",
    "        iniciales1 = ''.join([w[0] for w in str1.split() if w])\n",
    "        iniciales2 = ''.join([w[0] for w in str2.split() if w])\n",
    "        if iniciales1 == str2 or iniciales2 == str1:\n",
    "            abreviaciones.append((val1, val2))\n",
    "    return abreviaciones\n",
    "\n",
    "def agrupar_similares_automatico(valores, eps=0.3, ngram_range=(2,3)):\n",
    "    vec = TfidfVectorizer(analyzer='char_wb', ngram_range=ngram_range)\n",
    "    X = vec.fit_transform([str(v) for v in valores])\n",
    "    clustering = DBSCAN(eps=eps, min_samples=2, metric='cosine')\n",
    "    labels = clustering.fit_predict(X)\n",
    "    grupos = defaultdict(list)\n",
    "    for i, label in enumerate(labels):\n",
    "        if label != -1:\n",
    "            grupos[int(label)].append(valores[i])\n",
    "    return grupos\n",
    "\n",
    "# -------------------- FunciÃ³n integradora --------------------\n",
    "\n",
    "def diagnostico_avanzado_duplicados_allcols(\n",
    "    df,\n",
    "    tecnicas=('fuzzy', 'abrev', 'cluster'),\n",
    "    fuzzy_umbral=85,\n",
    "    max_unicos_auto=250,          # por encima de esto, se considera \"pesada\"\n",
    "    max_pairs_fuzzy=200_000,      # lÃ­mite duro de pares a evaluar con fuzzy\n",
    "    normalizar=True,              # normalizaciÃ³n textual previa\n",
    "    quitar_acentos=True,          # eliminar acentos en normalizaciÃ³n\n",
    "    top_resultados=3,             # cantidad de ejemplos para imprimir por tÃ©cnica\n",
    "    eps_cluster=0.3,              # parÃ¡metro de DBSCAN\n",
    "    ngram_range_cluster=(2,3)     # n-gramas para TF-IDF de clustering\n",
    "):\n",
    "    \"\"\"\n",
    "    DiagnÃ³stico avanzado de duplicados en TODAS las columnas categÃ³ricas.\n",
    "    - Evita cÃ¡lculos costosos en columnas con demasiados Ãºnicos (salta con aviso).\n",
    "    - Estima tiempo y limita pares a evaluar para fuzzy.\n",
    "    - Aplica normalizaciÃ³n (lower/strip/acentos) de forma opcional.\n",
    "    \"\"\"\n",
    "    columnas_cat = df.select_dtypes(include=['object']).columns\n",
    "    print(\"=== DIAGNÃ“STICO AVANZADO DE DUPLICADOS (todas las columnas categÃ³ricas) ===\\n\")\n",
    "\n",
    "    pesadas = []\n",
    "    for col in columnas_cat:\n",
    "        vals = df[col].dropna().unique().tolist()\n",
    "        n = len(vals)\n",
    "        print(f\"\\nðŸ” Columna: {col}  |  Ãºnicos: {n}\")\n",
    "\n",
    "        if normalizar:\n",
    "            vals_norm = _normalizar_valores(vals, quitar_acentos=quitar_acentos)\n",
    "        else:\n",
    "            vals_norm = [str(v) for v in vals]\n",
    "\n",
    "        # EstimaciÃ³n de costo\n",
    "        t_est = _estimacion_tiempo_pairs(n)\n",
    "        print(f\"   â±ï¸ EstimaciÃ³n rÃ¡pida (fuzzy ~O(nÂ²)): ~{t_est:.1f}s\")\n",
    "\n",
    "        # Salto automÃ¡tico si es muy pesada\n",
    "        if n > max_unicos_auto:\n",
    "            print(f\"   â­ï¸  Saltada: demasiados valores Ãºnicos (> {max_unicos_auto}).\")\n",
    "            pesadas.append((col, n))\n",
    "            continue\n",
    "\n",
    "        # TÃ©cnicas\n",
    "        if 'fuzzy' in tecnicas:\n",
    "            start = time.time()\n",
    "            # Limitar pares si hace falta\n",
    "            max_pairs = max_pairs_fuzzy\n",
    "            resultados = detectar_similares_fuzzy(vals_norm, umbral=fuzzy_umbral, max_pairs=max_pairs)\n",
    "            dur = time.time() - start\n",
    "            if resultados:\n",
    "                print(f\"   ðŸ“Š Fuzzy (â‰¥{fuzzy_umbral}%) â†’ {len(resultados)} pares  |  {dur:.1f}s\")\n",
    "                for v1, v2, sc in resultados[:top_resultados]:\n",
    "                    print(f\"      {sc}%: '{v1}' â‰ˆ '{v2}'\")\n",
    "            else:\n",
    "                print(f\"   ðŸ“Š Fuzzy: sin pares sobre el umbral  |  {dur:.1f}s\")\n",
    "\n",
    "        if 'abrev' in tecnicas:\n",
    "            start = time.time()\n",
    "            abrev = detectar_abreviaciones(vals_norm)\n",
    "            dur = time.time() - start\n",
    "            if abrev:\n",
    "                print(f\"   ðŸ”¤ Abreviaciones/contenimientos â†’ {len(abrev)} pares  |  {dur:.1f}s\")\n",
    "                for v1, v2 in abrev[:top_resultados]:\n",
    "                    print(f\"      '{v1}' â†” '{v2}'\")\n",
    "            else:\n",
    "                print(f\"   ðŸ”¤ Abreviaciones: no se detectaron  |  {dur:.1f}s\")\n",
    "\n",
    "        if 'cluster' in tecnicas:\n",
    "            start = time.time()\n",
    "            grupos = agrupar_similares_automatico(vals_norm, eps=eps_cluster, ngram_range=ngram_range_cluster)\n",
    "            dur = time.time() - start\n",
    "            if grupos:\n",
    "                print(f\"   ðŸŽ¯ Clustering (DBSCAN) â†’ {len(grupos)} grupos  |  {dur:.1f}s\")\n",
    "                for i, (g, items) in enumerate(grupos.items()):\n",
    "                    if i < top_resultados:\n",
    "                        print(f\"      Grupo {g}: {items}\")\n",
    "            else:\n",
    "                print(f\"   ðŸŽ¯ Clustering: no se formaron grupos  |  {dur:.1f}s\")\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    if pesadas:\n",
    "        print(\"\\nâš ï¸ Columnas saltadas por ser muy pesadas (ajusta max_unicos_auto si deseas incluirlas):\")\n",
    "        for col, n in pesadas:\n",
    "            print(f\"   â€¢ {col}: {n} Ãºnicos\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "diagnostico_avanzado_duplicados_allcols(\n",
    "     df,\n",
    "     tecnicas=('fuzzy', 'abrev', 'cluster'),\n",
    "     fuzzy_umbral=85,\n",
    "     max_unicos_auto=1000,\n",
    "     max_pairs_fuzzy=200_000,\n",
    "     normalizar=True,\n",
    "     quitar_acentos=True,\n",
    "     top_resultados=3,\n",
    "     eps_cluster=0.3,\n",
    "     ngram_range_cluster=(2,3)\n",
    " )\n",
    "\"\"\" Sugerencias:\n",
    "\n",
    "Ajusta max_unicos_auto segÃºn tu hardware. 250â€“400 suele ser razonable.\n",
    "\n",
    "Si quieres incluir â€œdocumentoâ€, sube max_unicos_auto pero baja max_pairs_fuzzy para acotar tiempo.\n",
    "\n",
    "La estimaciÃ³n es orientativa; puedes calibrar k_ref para tu equipo. \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
